{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: green;\"><center>Regularization</center></h1>\n",
    "Regularization in machine learning is a technique used to prevent overfitting by adding a penalty to the model's complexity. Overfitting occurs when a model learns not only the underlying patterns in the training data but also the noise, resulting in poor performance on new, unseen data. Regularization helps in creating simpler models that generalize better to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Common type of regularization</h3> \n",
    "\n",
    "1. L1 Regularization (Lasso)\n",
    "2. L2 Regularization (Ridge)\n",
    "3. Elastic Net Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>L1 Regularization (Lasso)</h3>\n",
    "\n",
    "- Mechanism: Adds the absolute values of the coefficients to the loss function.\n",
    "Cost Function = Loss + λ∑ ∣ Wi ∣\n",
    "\n",
    "Here, λ is the regularization parameter or hyperparameter it take any value for 0 to infinity, and are the model coefficients.\n",
    "\n",
    "\n",
    "- Effect: L1 regularization tends to produce sparse models, where some of the coefficients are exactly zero. This property makes Lasso useful for feature selection, as it effectively removes less important features.\n",
    "- Use Cases: When you want to perform feature selection or when you have a large number of features, and you suspect that many of them are irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>L2 Regularization (Ridge)</h3>\n",
    "\n",
    "- Mechanism: Adds the squared values of the coefficients to the loss function.\n",
    "Cost Function = Loss + λ ∑ (Wi)<sup>2</sup>\n",
    "\n",
    "- Effect: L2 regularization distributes the penalty among all coefficients, leading to smaller coefficients overall but rarely zeroing them out completely. It helps in stabilizing the model by reducing the variance.\n",
    "- Use Cases: When you have many features, and you believe that most of them contribute to the outcome, but you want to avoid large coefficients that might make the model sensitive to small changes in the input data\n",
    "\n",
    "​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Elastic Net Regularization</h3>\n",
    "\n",
    "- Mechanism: Combines both L1 and L2 penalties.\n",
    "Cost Function = Loss = + λ∑ ∣ Wi ∣ + λ ∑ (Wi)<sup>2</sup>\n",
    "- Effect: Elastic Net benefits from both L1 and L2 regularization, promoting sparsity while also maintaining some stability in the model.\n",
    "- Use Cases: When you have highly correlated features, Elastic Net can help by selecting groups of correlated features together. It is also useful when you want a balance between feature selection (L1) and coefficient shrinkage (L2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Benefits of Regularization</h3>\n",
    "\n",
    "1. Prevents Overfitting: By adding a penalty for large coefficients, regularization discourages the model from fitting the noise in the training data, leading to better generalization.\n",
    "2. Simplifies the Model: Especially with L1 regularization, unimportant features can be removed, resulting in a simpler and more interpretable model.\n",
    "3. Stabilizes Predictions: Regularization reduces the variance of the model's predictions, making them more stable and less sensitive to fluctuations in the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Mathematical Background</h3>\n",
    "In the context of linear regression, the standard objective function without regularization is:\n",
    "Cost Function=∑(yi - ŷi)<sup>2</sup>, \n",
    "where yi are the actual values and ŷi are the predicted values.\n",
    "\n",
    "With regularization, this becomes:\n",
    "\n",
    "- L1 Regularization:\n",
    "  Cost Function = ∑(yi - ŷi) + λ∑ ∣ Wi ∣\n",
    "\n",
    "- L2 Regularization:\n",
    "  Cost Function = ∑(yi - ŷi) +  λ ∑ (Wi)<sup>2</sup>\n",
    "\n",
    "- Elastic Net:\n",
    "  Cost Function = ∑(yi - ŷi) + λ∑ ∣ Wi ∣ + +  λ ∑ (Wi)<sup>2</sup>\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Implementation in Machine Learning Libraries</h3>\n",
    "Regularization is a standard feature in many machine learning libraries. For example:\n",
    "\n",
    "<h5>Scikit-Learn (Python):</h5>\n",
    "\n",
    "- Ridge Regression: sklearn.linear_model.Ridge\n",
    "- Lasso Regression: sklearn.linear_model.Lasso\n",
    "- Elastic Net: sklearn.linear_model.ElasticNet\n",
    "\n",
    "<h5>TensorFlow and Keras:</h5>\n",
    "\n",
    "- You can add L1, L2, or both penalties to layers using the kernel_regularizer parameter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
